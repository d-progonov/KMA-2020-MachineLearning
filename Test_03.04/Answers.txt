1. –ú–µ—Ç–æ–¥–∏ –∞–≥—Ä–µ–≥–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö (bagging) –ø—Ä–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—ñ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤.
	bootstrap aggregating (boosting) is a technique to reduce the variance of an estimate to average together many estimates. For example, we can train ùëÄ different trees on different subsets of the data, chosen randomly with replacement, and then compute the ensemble f(x) = sum(f_m(x))/M
	
2. –û–±—á–∏—Å–ª–∏—Ç–∏ –∞–Ω—Ç–∏–≥—Ä–∞–¥—ñ–µ–Ω—Ç –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –≤ —Ç–æ—á—Ü—ñ x = 0, y = 1:
  loss = 1/2 * (y - f(x)) ^2
  f(x) = tg(x^2 - 2) + exp(sin(x))
	
	–∞–Ω—Ç–∏–≥—Ä–∞–¥—ñ–µ–Ω—Ç - –≥—Ä–∞–¥—ñ—î–Ω—Ç –∑ –≤—ñ–¥'—î–º–Ω–∏–º –∑–Ω–∞–∫–æ–º
	–ú–∞—î–º–æ –≤–∑—è—Ç–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç –≤—ñ–¥ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –ø–æ —Ö —Ç–∞ y:
		loss_x = (y-f(x))*f'(x)_x, –¥–µ f'(x)_x= 2x*sec^2(x^2-2)+exp(sin(x))*cos(x)
		loss_y = (y - f(x))*1
		
		loss_x(0,1) = -2.185039863261519 * (0+1)=-2.185039863261519
		loss_y(0,1)= 1-3.18503 = -2.185039863261519
		
		loss_x(0,1)+loss_y(0,1) = -4.37
		
		antigrad = -(loss_x(0,1)+loss_y(0,1))=4.37